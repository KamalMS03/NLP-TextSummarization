{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f11c26c2",
      "metadata": {
        "id": "f11c26c2"
      },
      "source": [
        "# Summarization of Podcast Transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874ffae3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "874ffae3",
        "outputId": "cddb3361-f99d-460b-90f3-dd940d4cc210"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Introducing yourself to someone in a professional setting can be tricky, especially when facing an interview. Irrespective of your qualifications and experience, your way of self-introduction during an interview carries much weight when it comes to making a strong impression.As soon as you enter the room, exchange pleasantries and introduce yourself by saying your name. Keep this introduction short and concise before you go into detail when the interview starts.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#with open(\"/content/drive/MyDrive/podcast_text.txt\", \"r\") as file:\n",
        " #   text = file.read()\n",
        "#print(len(text))\n",
        "text = \"Introducing yourself to someone in a professional setting can be tricky, especially when facing an interview. Irrespective of your qualifications and experience, your way of self-introduction during an interview carries much weight when it comes to making a strong impression.As soon as you enter the room, exchange pleasantries and introduce yourself by saying your name. Keep this introduction short and concise before you go into detail when the interview starts.\"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6b6912",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a6b6912",
        "outputId": "d282d76f-1065-455c-a218-92109ee20513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.cluster.util import cosine_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13a96e6",
      "metadata": {
        "id": "e13a96e6"
      },
      "source": [
        "## TF-IDF Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f40457",
      "metadata": {
        "id": "38f40457"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce572776",
      "metadata": {
        "id": "ce572776"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "  tokens = word_tokenize(text.lower())\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7e3924",
      "metadata": {
        "id": "fd7e3924"
      },
      "outputs": [],
      "source": [
        "def get_tfidf_scores(text):\n",
        "  vectorizer = TfidfVectorizer(min_df=1)\n",
        "  try:\n",
        "    tf_idf_matrix = vectorizer.fit_transform([text])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    scores = dict(zip(feature_names, tf_idf_matrix.toarray()[0]))\n",
        "    return scores\n",
        "  except ValueError:\n",
        "    return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf60c264",
      "metadata": {
        "id": "cf60c264"
      },
      "outputs": [],
      "source": [
        "def summarize(text, num_sentences):\n",
        "    sentences = text.split(\".\")\n",
        "    processed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
        "    tfidf_scores = [get_tfidf_scores(\" \".join(sentence)) for sentence in processed_sentences]\n",
        "\n",
        "    sentence_scores = [sum(score.values()) / len(score) for score in tfidf_scores if len(score) != 0]\n",
        "\n",
        "    top_sentences = sorted(zip(sentences, sentence_scores), key=lambda x: x[1], reverse=True)[:num_sentences]\n",
        "    return \". \".join([sent for sent, _ in top_sentences])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6fa7de5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "c6fa7de5",
        "outputId": "23f66b4d-84be-45b7-dac5-2f133145e433"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" A delight to be here.  You're just overwhelmed by all of this.  You just give up.  You don't have any time at all.  And it's 80% shorter.  And to me, that's an example.  I expect that.  Yeah.  Right.  Yeah.  Yeah.  At 8 o'clock exactly, I was in there and I was out of there by 815.  We all know this.  You had to wait.  Think of that.  Exactly.  Yeah.  Beautiful.  Yeah.  Right\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "summary = summarize(text,20)\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eff1abf",
      "metadata": {
        "id": "0eff1abf"
      },
      "source": [
        "## Word2Vec Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b36c56b",
      "metadata": {
        "id": "3b36c56b"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7d4212",
      "metadata": {
        "id": "4d7d4212"
      },
      "outputs": [],
      "source": [
        "preprocessed_text = preprocess_text(text)\n",
        "word2vec_model = Word2Vec([preprocessed_text], min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9ba75e8",
      "metadata": {
        "id": "b9ba75e8"
      },
      "outputs": [],
      "source": [
        "def sentence_similarity(sentence1, sentence2):\n",
        "    s1 = preprocess_text(sentence1)\n",
        "    s2 = preprocess_text(sentence2)\n",
        "    s1_vector = sum(word2vec_model.wv[word] for word in s1) / len(s1)\n",
        "    s2_vector = sum(word2vec_model.wv[word] for word in s2 if word in word2vec_model.wv) / len(s2)\n",
        "\n",
        "    return cosine_similarity([s1_vector], [s2_vector])[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d18f6b",
      "metadata": {
        "id": "67d18f6b"
      },
      "outputs": [],
      "source": [
        "def extract_text_summary(text, num_sentences):\n",
        "    sentences = re.split('\\.|;|\\n', text)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    scores = []\n",
        "    for sentence in sentences:\n",
        "\n",
        "        scores.append((sentence, sentence_similarity(text, sentence)))\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    summary = [score[0] for score in scores[:num_sentences]]\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6161ec05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "6161ec05",
        "outputId": "89bea794-c9ad-497d-b6ff-dc0c4e1822a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"And that's one of the things that we like to think about friction is where can you put in good friction to stop bad friction? And, you know, so those solutions sort of adding one question, limiting it to four interviews, but still having four, not one, seem like they're finding this perfect balance of just enough friction, right? You know, you're not adding too much so that it becomes bad The minor but key wrinkle I'd like to touch on briefly, Alison, is if you're a middle manager, if you're the average Joe, even the top manager, it's easy to kind of think of the task as where do I take bad friction out? Instead, what we find is it's much better to focus on what the consequences and the consequences, giving employees the gift of time And to us, there's sort of a duality of friction fixers work 5 million people a year who want things like health insurance and food and stuff like that What about the role of the middle manager? You know, do you see adding and subtracting friction as a big part of that job? In the cultures that we work with that are good at friction fixing, I would actually to name some companies, I think that Amazon and Walmart are pretty good examples of this Is it a one way door decision or a two way door decision? In other words, what's the cost of reversing the decision? Is it very high or is it very low? If the cost of reversing a decision is very high, for example, an acquisition or hiring a C-suite executive, you really need a lot of good friction in the process And I like that because if you think about what a leader's job is, there are certain things in life, burdens, hassles, maybe a demanding customer you can't get rid of or the company will go out of business who's driving you crazy But they also need to think about sort of just helping people on an individual level to grease the wheels, as it were That's a frontline supervisor doing things within his cone of friction to make things easier on customers That's why what good friction does is those obstacles, they lead to deliberation, thought, and as you said, they help you make wiser decisions\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "summary = extract_text_summary(text,10)\n",
        "summary1 = \" \".join(sentence for sentence in summary)\n",
        "summary1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99f416f9",
      "metadata": {
        "id": "99f416f9"
      },
      "source": [
        "## Glove Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b2e566c",
      "metadata": {
        "id": "7b2e566c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f14ff12",
      "metadata": {
        "id": "4f14ff12"
      },
      "outputs": [],
      "source": [
        "def load_glove_model(file_path):\n",
        "    model = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            model[word] = vector\n",
        "    return model\n",
        "\n",
        "glove_model = load_glove_model(r\"/content/drive/MyDrive/glove.6B.50d.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdb1c132",
      "metadata": {
        "id": "cdb1c132"
      },
      "outputs": [],
      "source": [
        "def sentence_embedding(sentence):\n",
        "    words = preprocess_text(sentence)\n",
        "    word_vectors = [glove_model[word] for word in words if word in glove_model.keys()]\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8575e27",
      "metadata": {
        "id": "a8575e27"
      },
      "outputs": [],
      "source": [
        "def sentence_similarity(sentence1, sentence2):\n",
        "    s1_vector = sentence_embedding(sentence1)\n",
        "    s2_vector = sentence_embedding(sentence2)\n",
        "    if s1_vector is not None and s2_vector is not None:\n",
        "        return cosine_similarity([s1_vector], [s2_vector])[0][0]\n",
        "    else:\n",
        "        return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fa56485",
      "metadata": {
        "id": "0fa56485"
      },
      "outputs": [],
      "source": [
        "def extract_text_summary1(text, num_sentences):\n",
        "    sentences = re.split('\\.|;|\\n', text)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    scores = []\n",
        "    for sentence in sentences:\n",
        "        scores.append((sentence, sentence_similarity(text, sentence)))\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    summary = [score[0] for score in scores[:num_sentences]]\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b9826c6",
      "metadata": {
        "id": "7b9826c6",
        "outputId": "f6be1545-84af-4a61-9870-30c852b20f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The minor but key wrinkle I'd like to touch on briefly, Alison, is if you're a middle manager, if you're the average Joe, even the top manager, it's easy to kind of think of the task as where do I take bad friction out? Instead, what we find is it's much better to focus on what the consequences and the consequences, giving employees the gift of time And that's one of the things that we like to think about friction is where can you put in good friction to stop bad friction? And, you know, so those solutions sort of adding one question, limiting it to four interviews, but still having four, not one, seem like they're finding this perfect balance of just enough friction, right? You know, you're not adding too much so that it becomes bad And this might have made sense when they were hiring the first 100 people, 200 people, the company they were going to build them with What about the role of the middle manager? You know, do you see adding and subtracting friction as a big part of that job? In the cultures that we work with that are good at friction fixing, I would actually to name some companies, I think that Amazon and Walmart are pretty good examples of this I think you're beginning to get at this, but I'm interested to know a lot of us might accept friction as just a part of what the corporate world is But as we went lower down in the organization, employees lamented about how hard it was to get anything done in the first place When you hear the word friction, where does your mind go? Do you think of it as a force that leads to something positive and useful, like how rubbing two sticks together can create a fire? Or do you think of it as an enemy of progress, a slowdown mechanism, bureaucracy, conflict? Our guests today have spent the past seven years investigating friction in organizations They understand the work in the organization they lead so that they know what to make hard and what to make easy If you're an average worker and you see this going on, but your managers aren't recognizing it, how do you point out bad friction in a way that will make those higher ups listen? It depends how much psychological safety and job security you have And that's one of the things that also got us frustrated\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "summary = extract_text_summary1(text,10)\n",
        "summary2 = \" \".join(sentence for sentence in summary)\n",
        "summary2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Em4DNpsu-Nhp",
      "metadata": {
        "id": "Em4DNpsu-Nhp"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G7xsCDZM-Pg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164d5000-11a5-4d32-b83d-e79b9a726917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3819\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/drive/MyDrive/reff.txt\", \"r\") as file:\n",
        "    ref = file.read()\n",
        "print(len(ref))"
      ],
      "id": "1G7xsCDZM-Pg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rouge_score - Glove Embedding"
      ],
      "metadata": {
        "id": "NJSjneNbkwKG"
      },
      "id": "NJSjneNbkwKG"
    },
    {
      "cell_type": "code",
      "source": [
        "scores = scorer.score(sum, ref)\n",
        "scores"
      ],
      "metadata": {
        "id": "XDO6tUIiLRXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ff04ac-abe0-4256-8868-16ba0f8382d4"
      },
      "id": "XDO6tUIiLRXr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': Score(precision=0.38173652694610777, recall=0.6891891891891891, fmeasure=0.49132947976878605),\n",
              " 'rougeL': Score(precision=0.15269461077844312, recall=0.2756756756756757, fmeasure=0.19653179190751446)}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}